---
title: Benchmarks
layout: default
---

# Benchmarks

The benchmarks section of the website is divided into sections on simulators, models and papers. 

# Benchmark Submission

Submission of benchmark models is in two stages. Initial submission is less stringent than acceptance. It is encouraged to submit models early to engage in discussion with members of the community. It is expected that model acceptance will be the result of collaborative working to achieve the qualifying conditions of reference implementations and multi simulator benchmarking.

# First Stage Submission

In order to submit a benchmark model then please contact a chair or co-chair or directly submit a pull request via Github. In order to submit a model the following must be 

1) The model must be describable in high level description using text, equations or diagrams. It must be possible for the implementation of this model to be repeated by a reviewer.
2) The model must have a reference implementation at the time of submission. Before the next stage proposers of model will need to coordinate or provide a second reference implementation in another simulator. Any reference implementations must be available publicly. The licence does not matter as long as the community is free to implement the model and use outputs for non commercial purposes.
3) A description of how to statistically validate the model must be given so that reference implementations can be validated.
4) A textural description must identify which of the four scalability metrics are feasible within the model and propose how these can be scaled. E.g. Which parameters or model behaviours to vary and how.

# Second Stage Submission for Acceptance

All conditions for first stage acceptance must be met. Additionally;
1) The model must be peer reviewed by at least three active members of the OpenAB committee. Comments will be considered by the Chair and co-chairs in deeming the appropriateness of acceptance.
2) The model must have a reference implementation in at least 2 simulators.
3) Each reference simulation should be benchmarked and documented for publication on the website.

# Submission of a reference implementation of an existing benchmark

In order to submit a new implementation of a benchmark model please contact a chair or co-chair or directly submit a pull request via Github. Your submission should include;

1) The implementation code of the model. 
2) Documentation for how to execute your implementation in the given simulator or environment.
3) Benchmarking results for your implementation for publication on the website.


# Submission via Github

Details to follow...